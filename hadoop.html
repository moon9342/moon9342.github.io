<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Hadoop 설치와 활용</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />


    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
            src="https://code.jquery.com/jquery-3.2.1.min.js"
            integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
            crossorigin="anonymous">
    </script>

    <!-- 웹 폰트 설정 -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- Font Awesome CDN 설정 => 느림.. -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- custom.css -->
    <link rel="stylesheet" href="/assets/built/custom.css">

    <!-- syntax.css -->
    <link rel="stylesheet" href="/assets/built/syntax.css">

    <!-- gist.css -->
    <link rel="stylesheet" href="/assets/built/better-gist-styles.css">

    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--Dl Menu-->
    <script src="/assets/js/modernizr.custom.js"></script>
    <script src="/assets/js/jquery.dlmenu.js"></script>
    <script src="/assets/js/menu_main.js"></script>

    <!-- Google AdSense -->
    <script data-ad-client="ca-pub-2687024418565562" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="얼큰우동TV, 쉽게배우는 IT(Programming,Machine Learning,금융)" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/hadoop" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="IT Technical Training" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Hadoop 설치와 활용" />
    <meta property="og:description" content="VMware Workstation Pro 15 설치 및 CentOS 7 설치 Hadoop을 사용하기 위해서 Windows 10에 VMWARE Workstation Pro 15를 설치한 후 CentOS 7을 설치합니다. (VMWare Workstation Player는 무료로 사용가능하지만 Workstation Pro 버전은 유료입니다. 30일 trial을 이용할 수 있습니다.) 먼저 VMware Workstation Pro를 설치하고 실행하면 다음과 같은 화면을 볼 수 있습니다. 새로운" />
    <meta property="og:url" content="http://localhost:4000/hadoop" />
    <meta property="og:image" content="http://localhost:4000/assets/built/images/iot-logo.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2020-02-02T01:40:00+09:00" />
    <meta property="article:modified_time" content="2020-02-02T01:40:00+09:00" />
    <meta property="article:tag" content="Et-cetera" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Hadoop 설치와 활용" />
    <meta name="twitter:description" content="VMware Workstation Pro 15 설치 및 CentOS 7 설치 Hadoop을 사용하기 위해서 Windows 10에 VMWARE Workstation Pro 15를 설치한 후 CentOS 7을 설치합니다. (VMWare Workstation Player는 무료로 사용가능하지만 Workstation Pro 버전은 유료입니다. 30일 trial을 이용할 수 있습니다.) 먼저 VMware Workstation Pro를 설치하고 실행하면 다음과 같은 화면을 볼 수 있습니다. 새로운" />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/built/images/iot-logo.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="IT Technical Training" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Et-cetera" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "IT Technical Training",
        "logo": "http://localhost:4000/"
    },
    "url": "http://localhost:4000/hadoop",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/built/images/iot-logo.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/hadoop"
    },
    "description": "VMware Workstation Pro 15 설치 및 CentOS 7 설치 Hadoop을 사용하기 위해서 Windows 10에 VMWARE Workstation Pro 15를 설치한 후 CentOS 7을 설치합니다. (VMWare Workstation Player는 무료로 사용가능하지만 Workstation Pro 버전은 유료입니다. 30일 trial을 이용할 수 있습니다.) 먼저 VMware Workstation Pro를 설치하고 실행하면 다음과 같은 화면을 볼 수 있습니다. 새로운"
}
    </script>

    <!-- <script type="text/et-cetera" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/et-cetera">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <!-- 사용자 browser 확인 코드 - Chrome이 아니면 경고 메시지 출력 -->

    <script>
        var agent = navigator.userAgent.toLowerCase();
        var _iPhone = false;
        var _chrome = false;
        var result = false;
        if (agent.indexOf("iphone") != -1) {
            _iPhone = true;
        }
        if (agent.indexOf("chrome") != -1) {
            _chrome = true;
        }

        if( _iPhone || _chrome ) {
            result = true;
        }

        if( !result ) {
            alert("이 블로그는 Google Chrome Browser에 최적화 되어 있습니다 !!");
        }

    </script>

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Hadoop 설치와 활용" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000">IT Technical Training</a>
            
        
        
            
<ul class="nav" role="menu">
<!--    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>-->
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
<!--
    <li class="nav-data-analysis-article" role="menuitem"><a href="/author/data-analysis/">Data Analysis(R)</a></li>
    <li class="nav-python-article" role="menuitem"><a href="/author/python/">Python</a></li>
    <li class="nav-python-article" role="menuitem"><a href="/tag/jekyll/">Jekyll</a></li>
    <li class="nav-miscellaneous-article" role="menuitem"><a href="/author/fragmentary-knowledge/">Miscellaneous Article</a></li>
-->
<!--
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
-->
    <li class="nav-python" role="menuitem">
        <a href="/tag/jekyll/">Jekyll Blog 생성</a>
    </li>
    <li class="nav-python" role="menuitem">
        <a href="/tag/python/">Python</a>
    </li>
    <li class="nav-data-science" role="menuitem">
        <a href="/tag/data-science/">Data Science</a>
    </li>
    <li class="nav-machine-learning" role="menuitem">
        <a href="/tag/machine-learning/">Machine Learning</a>
    </li>
    <li class="nav-python" role="menuitem">
        <a href="/tag/r/">R 기초강의</a>
    </li>
<!--    <li class="nav-python" role="menuitem">-->
<!--        <a href="/tag/angular/">Angular</a>-->
<!--    </li>-->
    <li class="nav-python" role="menuitem">
        <a href="/tag/et-cetera/">Et cetera</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">All Posts</a>
    </li>
</ul>

<!-- 아래 내용이 있으면 홈 화면 좌측 상단에 Navigation 버튼이 생성됨.. 굳이 필요없어서 comment 처리 -->

<!--
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
    <button class="dl-trigger">Open Menu</button>
    <ul class="dl-menu">
        <li><a href="http://localhost:4000/">Home</a></li>
        <li>
            <a href="#">About</a>
            <ul class="dl-submenu">
                <li style="text-align: center">
                    <img src="http://localhost:4000/assets/built/images/photo.jpg" alt="SungHoon Moon photo" class="author-photo">
                </li>
                <li>
                    <h4>SungHoon Moon</h4>
                    <p>Director of ATGLab. <br>Ph.D. in Computer Science</p>
                </li>
                <li><a href="http://localhost:4000/about/"><i class="fa fa-external-link"></i><span class="btn btn-inverse"> Learn More</span></a></li>
                <li>
                <a href="mailto:moon9342@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
            </li>
                
                
                
                
                
                
                
                
                
            </ul>
        </li>
        <li>
            <a href="#">Posts</a>
            <ul class="dl-submenu">
                <li><a href="http://localhost:4000/archive">All Posts</a></li>
                <li><a href="http://localhost:4000/author_archive">All Tags</a></li>
            </ul>
        </li>
        
    </ul>
</nav>
-->
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post tag-front-end ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 1 February 2020"> 1 February 2020</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/et-cetera/'>ET-CETERA</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">Hadoop 설치와 활용</h1>
            </header>

            
<!--
            post에 cover image를 표시하지 않게 설정
            <figure class="post-full-image" style="background-image: url(/assets/built/images/iot-logo.png)">
            </figure>
-->
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p><strong class="subtitle_fontAwesome">VMware Workstation Pro 15 설치 및 CentOS 7 설치</strong></p>

<p>Hadoop을 사용하기 위해서 Windows 10에 <code class="highlighter-rouge">VMWARE Workstation Pro 15</code>를 설치한 후 <code class="highlighter-rouge">CentOS 7</code>을 설치합니다.
(VMWare Workstation Player는 무료로 사용가능하지만 Workstation Pro 버전은 유료입니다. 30일 trial을 이용할 수 있습니다.)</p>

<p>먼저 VMware Workstation Pro를 설치하고 실행하면 다음과 같은 화면을 볼 수 있습니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-1.png" alt="VMWARE 설치-1" /></p>

<p>새로운 가상머신을 만듭니다. 가상머신을 만든 후 나중에 iso파일을 이용해서 CentOS 7을 설치합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-2.png" alt="VMWARE 설치-2" /></p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-3.png" alt="VMWARE 설치-3" /></p>

<p>가상머신의 이름을 정해주는 부분이 나오는데 우리는 가상머신을 총 4개 이용할것입니다. 각각의 가상머신 이름을
Hadoop01 ~ Hadoop04까지 사용합니다. 처음의 가상머신만 생성하고 나머지 3개의 가상머신은 clone해서 사용합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-4.png" alt="VMWARE 설치-4" /></p>

<p>디스크 공간은 50GB로 설정하고 <code class="highlighter-rouge">Customize Hardware</code>를 클릭해서 사용 메모리는 2GB로 설정합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-5.png" alt="VMWARE 설치-5" /></p>

<p>생성이 완료된 후 Virtual Machine Setting을 이용하여 <code class="highlighter-rouge">CD/DVD</code>부분에 <code class="highlighter-rouge">CentOS 7 iso image파일</code>을 연결한 후
Virtual Machine을 기동시키면 CentOS 7 설치가 진행됩니다.</p>

<p>CentOS 7 iso image는 <a href="http://mirror.kakao.com/centos/7.8.2003/isos/x86_64/" target="_blank">여기</a>를 
클릭하면 <code class="highlighter-rouge">CentOS-7-x86_64-DVD-2003.iso</code> 파일을 받으실 수 있습니다.</p>

<p>아래의 화면은 VMware Player를 이용한 설치화면입니다. WMware Workstation을 사용하는것과 차이가 없습니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-6.png" alt="VMWARE 설치-6" /></p>

<p>먼저 언어선택에서 <code class="highlighter-rouge">한국어</code>를 선택하시고 계속 진행하면 다음과 같은 화면이 나오는데 여기서 먼저 
<code class="highlighter-rouge">네트워크 및 호스트명</code>으로 들어간 후 네크워크를 연결(켬)합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-7.png" alt="VMWARE 설치-7" /></p>

<p><code class="highlighter-rouge">완료</code>를 클릭하신 후 <code class="highlighter-rouge">소프트웨어 &gt; 소프트웨어 설치</code>를 클릭한 후 다음과 같이 설정합니다. 
(<code class="highlighter-rouge">GNOME</code> 사용이 필수는 아닙니다. 사용의 편의를 위해서 GNOME을 선택한 것 뿐입니다.)</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-8.png" alt="VMWARE 설치-8" /></p>

<p>완료를 클릭한 후 <code class="highlighter-rouge">시스템 &gt; 설치대상</code>을 선택하여 파티션을 잡아줍니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-9.png" alt="VMWARE 설치-9" /></p>

<p><code class="highlighter-rouge">파티션을 설정합니다.</code> 부분을 선택하고 완료를 누르면 수동으로 파티션 설정하는 부분으로 넘어갑니다. 다음의
각 파티션을 설정합니다.</p>

<ul>
  <li><code class="highlighter-rouge">swap</code> : 2GB</li>
  <li><code class="highlighter-rouge">/boot</code> : 1GB</li>
  <li><code class="highlighter-rouge">/home</code> : 10GB</li>
  <li><code class="highlighter-rouge">/</code> : 용량을 지정하지 않으면 나머지 모든 용량을 자동으로 할당합니다.</li>
</ul>

<p><img src="../../assets/built/images/iot-hadoop-vmware-10.png" alt="VMWARE 설치-10" /></p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-11.png" alt="VMWARE 설치-11" /></p>

<p>설치시작을 눌러 설치를 시작합니다.</p>

<p>root 유저의 암호를 변경하고 새로운 사용자를 한명 추가합니다. 새로운 사용자를 추가했지만
사용의 편의를 위해 root 계정을 이용하도록 하겠습니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-12.png" alt="VMWARE 설치-12" /></p>

<p>설치가 끝나면 재부팅하고 root 계정으로 로그인합니다.</p>

<p><strong class="subtitle2_fontAwesome">Java 설치</strong></p>

<p>Hadoop은 JVM상에서 동작하기 때문에 Java를 설치해야 합니다. Java를 다시 설치하고 환경변수를 설정합니다. 
Java는 8버전을 이용합니다.</p>

<ul>
  <li>기존의 Java 삭제</li>
</ul>

<blockquote>
  <p><strong>rpm -qa | grep java</strong></p>
</blockquote>

<p>결과로 출력된 파일 중 <code class="highlighter-rouge">javapackages-tools-3.4.1-11.el7.noarch</code>, <code class="highlighter-rouge">python-javapackages-3.4.1-11.el7.noarch</code>,
<code class="highlighter-rouge">tzdata-java-2018e-3.el7.noarch</code> 3개에 대해서 삭제를 진행합니다.</p>

<blockquote>
  <p><strong>yum remove javapackages-tools-3.4.1-11.el7.noarch</strong></p>
</blockquote>

<blockquote>
  <p><strong>yum remove python-javapackages-3.4.1-11.el7.noarch</strong></p>
</blockquote>

<blockquote>
  <p><strong>yum remove tzdata-java-2018e-3.el7.noarch</strong></p>
</blockquote>

<blockquote>
  <p><strong>rpm -qa | grep java</strong></p>
</blockquote>

<p>설치된 내용이 없음을 확인한 후 Oracle에서 Java 8을 다운로드 합니다.
만약 다운로드가 잘 안되면 Windows에서 다운로드 한 후 (Linux X64 tar.gz파일) 공유폴더를 이용해서
CentOS로 복사합니다.</p>

<p>공유폴더 설정은 윈도우에서 공유폴더를 하나 생성한 후 폴더의 <code class="highlighter-rouge">속성 &gt; 공유 &gt; 고급공유</code>에서 <code class="highlighter-rouge">선택한 폴더공유</code>를
체크하고 <code class="highlighter-rouge">권한</code>부분에서 <code class="highlighter-rouge">eveyone에 모든 권한</code>을 부여합니다.</p>

<p>VMware 가상머신 설정에서 <code class="highlighter-rouge">option</code> 탭에서 아래의 그림과 같이 설정합니다. 설정이 끝나면 <code class="highlighter-rouge">VMware Tool</code>을 설치합니다.
<code class="highlighter-rouge">VM</code> 메뉴의 <code class="highlighter-rouge">Install VMware Tools</code>를 선택하면 VMware Tools가 CD형식으로 바탕화면에 나타나고 
<code class="highlighter-rouge">/run/media/root/VMware Tools/</code> 디렉토리로 이동하면 <code class="highlighter-rouge">VMwareTools-10.3.21-14772444.tar.gz</code> 파일을 확인할 수 있습니다.</p>

<p>해당 파일을 root 계정의 home으로 복사한 후 아래의 명령을 이용하여 해당 파일의 압축을 풀고 
설치 shell script를 실행하면 VMware tool을 설치할 수 있습니다.</p>

<blockquote>
  <p><strong>tar zxvf VMwareTools-10.3.21-14772444.tar.gz</strong></p>
</blockquote>

<p>VMware Tool가 설치되어야 <code class="highlighter-rouge">/mnt/hgfs</code>폴더가 생성되고 그 안에 윈도우에서 공유한 공유폴더가 보여지게 됩니다..</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-sharedfolder.png" alt="공유폴더 설정" /></p>

<p>Java 파일을 얻었으면 압축을 풀고 <code class="highlighter-rouge">/usr/local/java</code> 폴더로 <code class="highlighter-rouge">Java Home</code>을 설정합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-java-install.png" alt="Java 설치" /></p>

<p>vi를 이용해서 /etc/profile에 다음의 내용을 추가합니다.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export JAVA_HOME=/usr/local/java
export HADOOP_HOME=/usr/local/hadoop
export CLASSPATH=$JAVA_HOME/lib:$CLASSPATH
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</code></pre></div></div>

<p>변경된 profile을 적용시켜야 합니다.</p>

<blockquote>
  <p><strong>source /etc/profile</strong></p>
</blockquote>

<p>Java가 정상적으로 설치됬는지 확인합니다.</p>

<blockquote>
  <p><strong>java -version</strong></p>
</blockquote>

<p><img src="../../assets/built/images/iot-hadoop-java-version.png" alt="Java Version" /></p>

<hr />

<p><strong class="subtitle_fontAwesome">VMWARE CentOS 7 복사</strong></p>

<p>Hadoop을 사용하기 위해서 서버 4개를 이용할 것입니다. 설치가 끝나면 생성된 가상머신에서 마우스 오른쪽
클릭을 하면 메뉴가 나오는데 <code class="highlighter-rouge">Manage &gt; Clone...</code>을 선택합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-13.png" alt="VMWARE 설치-13" /></p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-14.png" alt="VMWARE 설치-14" /></p>

<p><code class="highlighter-rouge">Create a full clone</code>을 선택 후 가상 머신 이름을 입력합니다. Hadoop02 ~ Hadoop04까지 3개를 생성합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-15.png" alt="VMWARE 설치-15" /></p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-16.png" alt="VMWARE 설치-16" /></p>

<p><img src="../../assets/built/images/iot-hadoop-vmware-17.png" alt="VMWARE 설치-17" /></p>

<hr />

<p><strong class="subtitle_fontAwesome">Hadoop</strong></p>

<p>Hadoop은 Apache 최상위 오픈소스 프로젝트로 대용량의 데이터를 저장하고 분석하고 처리하기 위한 framework입니다.</p>

<p><strong class="subtitle2_fontAwesome">Hadoop의 특징은 다음과 같습니다.</strong></p>

<ul>
  <li>
    <p><strong>선형적인 확장성 제공</strong> : 일반적인 스토리지는 초기에 애플리케이션에서 사용할 용량을 예측하여 미리 스토리지 용량을 
확보한 상태에서 시스템이 오픈되나, Hadoop을 이용할 경우에는 서비스 초기에 필요한 수준으로만 
스토리지 용량을 확보해 시스템을 오픈한 후 데이터 증가 추이를 보면서 스토리지를 추가하는 방식으로 진행합니다.</p>
  </li>
  <li>
    <p><strong>데이터 분석 처리에 활용</strong> : 분석용 데이터를 HDFS에 저장하고, Map/Reduce라는 분산/병렬 처리 프레임워크를 
데이터 분석하는데 사용합니다.</p>
  </li>
  <li>
    <p><strong>API 기반의 파일 처리 시스템</strong> : 일반적인 파일시스템처럼 운영체제에서 제공하는 파일처리 명령을 이용할 수 없고, 
Hadoop을 이용하는 애플리케이션은 Hadoop에서 제공하는 명령어나 프로그램 API를 이용해야 합니다.</p>
  </li>
  <li>
    <p><strong>Immutable File System</strong> : 파일은 한번 써지면 변경되지 않는다고 가정합니다. 따라서 Hadoop은 파일을 
저장하고 저장된 파일에 대해 읽기 요청 위주(스트리밍 방식)인 응용이나 배치 작업 등에 적합합니다. 
이런 제약 때문에 파일 처리를 주로 하는 기존 솔루션이나 시스템을 수정없이 Hadoop에 적용 할 수는 없습니다.</p>
  </li>
  <li>
    <p><strong>네임스페이스 관리를 NameNode 메모리에 저장</strong> : 파일 시스템의 네임스페이스 정보를 NameNode의 메모리상에서 
관리하기 때문에 Hadoop에 저장할 수 있는 파일과 디렉토리의 개수는 NameNode의 메모리 크기에 제한을 받게됩니다.</p>
  </li>
  <li>
    <p><strong>NameNode 이중화</strong> : NameNode가 SPOF(Single Point Of Failre)입니다. NameNode에 문제가 발생하면 파일시스템 
전체 클러스터에 장애가 발생하게 됩니다. 따라서 일반적으로 Secondary NameNode를 사용해야 합니다.</p>
  </li>
</ul>

<p><strong class="subtitle2_fontAwesome">하둡 분산형 파일시스템(Hadoop Distributed FileSystem, HDFS) 특징</strong></p>

<ul>
  <li>
    <p>HDFS는 데이터를 저장하면 다수의 노드에 복제 데이터도 함께 저장하여 데이터의 유실을 방지합니다.</p>
  </li>
  <li>
    <p>HDFS에 파일을 저장하거나, 저정된 파일을 조회하려면 스트리밍 방식으로 접근해야 합니다.</p>
  </li>
  <li>
    <p>한번 저장된 파일은 수정할 수 없고(2.0부터는 append가능) 읽기만 가능합니다. 따라서 데이터의 무결성을 유지할 수 있습니다.</p>
  </li>
  <li>
    <p>데이터의 수정은 불가능하지만 파일이동, 삭제, 복사를 할 수 있는 인터페이스는 제공합니다.</p>
  </li>
</ul>

<p><strong class="subtitle2_fontAwesome">하둡 분산형 파일시스템(Hadoop Distributed FileSystem, HDFS) 아키텍쳐</strong></p>

<p><img src="../../assets/built/images/iot-hadoop-architecture.png" alt="HDFS Architecture" /></p>

<ul>
  <li>
    <p>블록 구조의 파일 시스템으로, 저장하는 파일은 특정 사이즈의 블록으로 나눠져 분산된 서버에 저장됩니다.</p>
  </li>
  <li>
    <p>기본적으로 HDFS 블록의 사이즈는 Hadoop 배포판에 따라서 64,128,256MB로 상이합니다. 일반적으로 물리적인
디스크는 블록이라는 개념을 이용합니다. 이 블록은 한번에 읽고 쓸 수 있는 데이터의 최대량을 의미합니다. 보통의
파일 시스템의 블록크기는 수 KB이지만 Hadoop의 HDFS는 128MB와 같이 굉장히 큰 단위입니다.
이렇게 큰 단위를 사용하는 이유는 데이터 탐색에 대한 비용을 최소화하기 위해서 입니다. 블록이 크면 블록의 시작점을
탐색하는데 시간이 적게 걸리고 데이터를 전송하는데 더 많은 시간을 할당해 줄 수 있기 때문입니다.
또한 HDFS 파일은 블록 크기보다 작은 데이터일 경우 전체 블록 크기에 해당하는 하위 디스크를 모두 점유하지는 않습니다.
즉, 블록의 크기가 128MB이고 1MB의 데이터를 저장한다면 128MB의 디스크를 사용하는게 아니라 1MB의 디스크만을 사용합니다.</p>
  </li>
  <li>
    <p>하나의 블록은 기본적으로 3개(변경가능)로 복제되며 각각 다른 HDFS의 노드에 분산저장됩니다.</p>
  </li>
  <li>
    <p>HDFS는 <code class="highlighter-rouge">Master</code> 역할을 하는 <code class="highlighter-rouge">NameNode</code> 서버 한 대와, <code class="highlighter-rouge">Slave</code> 역할을 하는 <code class="highlighter-rouge">DataNode</code> 서버 여러 대로 구성됩니다.</p>
  </li>
  <li>
    <p>NameNode는 HDFS의 모든 메타데이터(블록들이 저장되는 디렉토리의 이름, 파일명등)를 관리하고, HDFS 클라이언트가 이를 
이용하여 저장된 파일에 접근할 수 있습니다.</p>
  </li>
  <li>
    <p>Hadoop 어플리케이션은 HDFS에 파일을 저장하거나, 저장된 파일을 읽기 위해 HDFS 클라이언트를 사용하며, 
클라이언트는 API형태로 사용자에게 제공합니다.</p>
  </li>
  <li>
    <p>DataNode는 주기적으로 NameNode에게 블록 리포트(노드에 저장되어 있는 블록의 정보)를 전송하고 이를 통해 
NameNode는 DataNode가 정상 동작하는지 확인합니다.</p>
  </li>
  <li>
    <p>HDFS 클라이언트는 NameNode에 접속해서 원하는 파일이 저장된 블록의 위치를 확인하고, 
해당 블록이 저장된 DataNode에서 직접 데이터를 조회합니다.</p>
  </li>
</ul>

<p><strong class="subtitle2_fontAwesome">하둡 분산형 파일시스템(Hadoop Distributed FileSystem, HDFS) 파일 저장 Flow</strong></p>

<p><img src="../../assets/built/images/iot-hadoop-save-data-flow.png" alt="HDFS File Save Flow" /></p>

<ol>
  <li>
    <p>어플리케이션이 HDFS 클라이언트에게 파일 저장을 요청하면, 
HDFS 클라이언트는 NameNode에게 파일 블록들이 저장될 경로 생성을 요청합니다. 
NameNode는 해당 파일 경로가 존재하지 않으면 경로를 생성한 후, 다른 클라이언트가 해당 경로를 수정하지 못하도록 
Lock을 설정합니다. 그 후, NameNode는 클라이언트에게 해당 파일 블록들을 저장할 DataNode의 목록을 반환합니다.</p>
  </li>
  <li>
    <p>클라이언트는 첫 번째 DataNode에게 데이터를 전송합니다.</p>
  </li>
  <li>
    <p>첫 번째 DataNode는 데이터를 로컬에 저장한 후, 데이터를 두 번째 DataNode로 전송합니다.</p>
  </li>
  <li>
    <p>두 번째 DataNode는 데이터를 로컬에 저장한 후, 데이터를 세 번째 DataNode로 전송합니다.</p>
  </li>
  <li>
    <p>로컬에 데이터를 저장하였으면 자기에게 데이터를 넘겨준 DataNode에게 데이터의 로컬 저장이 
완료 되었음을 응답(Ack)합니다.</p>
  </li>
  <li>
    <p>로컬에 데이터를 저장하였으면 자기에게 데이터를 넘겨준 DataNode에게 데이터의 로컬 저장이 
완료 되었음을 응답(Ack)합니다.</p>
  </li>
  <li>
    <p>첫 번째 DataNode는 클라이언트에게 파일 저장이 완료 되었음을 응답합니다.</p>
  </li>
</ol>

<p><strong class="subtitle2_fontAwesome">하둡 분산형 파일시스템(Hadoop Distributed FileSystem, HDFS) 파일 읽기 Flow</strong></p>

<p><img src="../../assets/built/images/iot-hadoop-load-data-flow.png" alt="HDFS File Load Flow" /></p>

<ol>
  <li>
    <p>어플리케이션이 클라이언트에게 파일 읽기를 요청합니다.</p>
  </li>
  <li>
    <p>클라이언트는 NameNode에게 요청된 파일이 어떤 블록에 저장되어 있는지 정보를 요청합니다.</p>
  </li>
  <li>
    <p>메타데이터를 통해 파일이 저장된 블록 리스트를 반환합니다.</p>
  </li>
  <li>
    <p>클라이언트는 DataNode에 접근하여 블록 조회 요청합니다.</p>
  </li>
  <li>
    <p>DataNode는 클라이언트에게 요청된 블록을 전송합니다.</p>
  </li>
  <li>
    <p>클라이언트를 어플리케이션에 데이터를 전달합니다.</p>
  </li>
</ol>

<p><strong class="subtitle2_fontAwesome">MapReduce(맵리듀스)</strong></p>

<p>대용량의 데이터처리를 위한 분산 프로그래밍 모델이자 software framework입니다. 대량의 데이터를
병렬로 분석할 수 있으며 프로그래머는 Map과 Reduce라는 두 개의 method를 직접 작성해서 구성합니다.</p>

<ul>
  <li>
    <p><strong>Map</strong> : 흩어져 있는 데이터를 연관성이 있는 데이터로 key,value형태로 분류하는 작업을 지칭합니다.</p>
  </li>
  <li>
    <p><strong>Reduce</strong> : Map에서 출력된 데이터에서 중복된 데이터를 제거하고 원하는 데이터를 추출하른 작업을 수행합니다.</p>
  </li>
</ul>

<p>아래의 그림은 문자열 데이터에 포함된 단어의 빈도수를 출력해주는 과정입니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-mapreduce.png" alt="MapReduce" /></p>

<ol>
  <li>
    <p><strong>Splitting</strong> : 문자열 데이터를 라인별로 나눕니다.</p>
  </li>
  <li>
    <p><strong>Mapping</strong> : 라인별로 문자열을 입력받아, &lt;key, value&gt; 형태로 출력합니다.</p>
  </li>
  <li>
    <p><strong>Intermediate Splitting(Shuffling)</strong> : 같은 key를 가지는 데이터끼리 분류합니다.</p>
  </li>
  <li>
    <p><strong>Reducing</strong> : 각 key 별로 빈도수를 합산해서 출력합니다.</p>
  </li>
  <li>
    <p><strong>Combining</strong> : Reduce 메소드의 출력 데이터를 합쳐서 HDFS에 저장합니다.</p>
  </li>
</ol>

<hr />

<p><strong class="subtitle_fontAwesome">CentOS 7에 Hadoop 설치</strong></p>

<p>4대의 서버로 진행합니다.</p>

<ul>
  <li><strong>Hadoop01</strong> : NameNode (host명 : namenode)</li>
  <li><strong>Hadoop02</strong> : DataNode (host명 : datanode01)</li>
  <li><strong>Hadoop03</strong> : DataNode (host명 : datanode02)</li>
  <li><strong>Hadoop04</strong> : DataNode (host명 : datanode03)</li>
</ul>

<p>Hadoop의 설치 모드는 총 3개가 있습니다.</p>

<ul>
  <li><strong>Standalone</strong> : 단일 Node로 사용합니다. 테스트 및 디버깅 용도입니다.</li>
  <li><strong>Pseudo distributed</strong> : 단일 Node에서 Cluster를 구성합니다.</li>
  <li><strong>Full distributed</strong> : 두 대 이상의 Node를 CLuster로 구성합니다.</li>
</ul>

<p>여기서는 3번째 <code class="highlighter-rouge">Full distributed</code> 방식으로 진행하며 Hadoop의 버전은 <code class="highlighter-rouge">2.9.2</code>버전을 사용합니다.</p>

<p><strong class="subtitle2_fontAwesome">참고 : 암호화 방식</strong></p>

<ul>
  <li>대칭키 암호화 방식</li>
</ul>

<p>암호화를 위해서는 기본적으로 암호화의 대상이 되는 평서문(Plain Text)과 암호화를 위한 일종의 비밀번호인 
암호키(Cryptography Key) 그리고 마지막으로 암호화 알고리즘(Algorithm)이 필요한데, 
대칭키 암호화 방식은 문자 그대로 암호화와 복호화를 동일한 암호키를 이용하는 방식입니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-enc-dec.png" alt="대칭키 암호화 방식" /></p>

<p>위의 그림을 통해 살펴보면 동일한 암호키를 통해 암호화(Encryption), 복호화(Decryption)하는 것을 볼 수 있는데, 
대칭키를 사용하는 암호화 알고리즘은 <code class="highlighter-rouge">AES</code>와 <code class="highlighter-rouge">DES</code>가 대표적입니다.</p>

<ul>
  <li>RSA를 이용한 암호화</li>
</ul>

<p><code class="highlighter-rouge">RSA</code>는 공개키를 이용하는 대표적인 암호화 방식이며 전자서명이 가능한 최초의 알고리즘으로 알려져 있습니다. 
AES, DES와 같은 대칭키 암호화 방식에서 발생하는 문제점을 해결하였습니다. 과거의 암호 방식은 암호화를 위한 
키뿐만 아니라 알고리즘 역시 노출되지 않기 위해 노력하였으나 현대의 암호에서는 알고리즘을 공개하도록 
하고 있습니다. 키 이외에 암호 시스템의 모든 것이 공개되어도 안전해야 한다고 Kerckhoff라는 사람이 주장 했는데 
이것을 <code class="highlighter-rouge">Kerckhoff의 법칙</code>이라고 합니다.</p>

<p>RSA는 수학적인 기법을 통해 한 쌍의 <code class="highlighter-rouge">공개키</code>와 <code class="highlighter-rouge">비밀키</code>를 생성하는데, 각각의 키는 이론적으로 다음과 같은 용도로
사용됩니다.</p>

<p><code class="highlighter-rouge">Public Key</code> : 누구에게나 공개될 수 있으며 메세지를 보내는 발신자는 공개키를 통해 정보를 암호화한다.</p>

<p><code class="highlighter-rouge">Private Key</code> : 수신자는 비밀키를 암호화된 메세지를 복호화 하는데 사용한다. 외부에 노출되지 않도록 안전하게 보관해야 한다.</p>

<p>이와 같이 <code class="highlighter-rouge">RSA</code>를 이용한 공개키 암호화 방식은 비밀키(Private Key)를 외부에 노출할 위험이 없어 기존의 대칭키 
암호화 방식의 문제를 해결할 수 있습니다.</p>

<p><strong class="subtitle2_fontAwesome">SSH 설정</strong></p>

<p>각 서버의 IP address를 알아야 합니다. <code class="highlighter-rouge">ifconfig</code>를 이용하여 각 Host의 ip address를 파악합니다.</p>

<ul>
  <li>Hadoop01 : 192.168.64.128</li>
  <li>Hadoop02 : 192.168.64.129</li>
  <li>Hadoop03 : 192.168.64.130</li>
  <li>Hadoop04 : 192.168.64.131</li>
</ul>

<p>각 서버마다 /etc/hosts 파일을 편집해서 아래의 내용을 넣어줍니다.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>192.168.64.128 namenode
192.168.64.129 datanode01
192.168.64.130 datanode02
192.168.64.131 datanode03
</code></pre></div></div>

<p>각 서버마다 /etc/hostname 파일을 편집해서 각 host의 이름을 입력합니다.</p>

<ul>
  <li>192.168.64.128 =&gt; namenode</li>
  <li>192.168.64.129 =&gt; datanode01</li>
  <li>192.168.64.130 =&gt; datanode02</li>
  <li>192.168.64.131 =&gt; datanode03</li>
</ul>

<p>수정이 끝났으면 재부팅을 해줍니다.(서비스를 다시 시작해도 됩니다.)</p>

<p>Hadoop은 클러스터간에 내부 통신에 <code class="highlighter-rouge">SSH(Secure Shell)</code> 프로토콜을 이용합니다. 따라서 패스워드 인증과정없이
SSH 통신을 하기 위해 SSH 공개키를 설정하고 모든 서버에 통합 공개키를 복사하여 통신을 합니다.</p>

<p>각 서버에서 인증키를 생성합니다.</p>

<blockquote>
  <p><strong>ssh-keygen</strong></p>
</blockquote>

<p>입력을 요구하는 부분이 있는데 그냥 <code class="highlighter-rouge">enter</code>를 입력해도 무방합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-ssh-keygen.png" alt="대칭키 암호화 방식" /></p>

<p>모든 서버에서 이 작업을 수행하면 각 서버의 <code class="highlighter-rouge">/root/.ssh/</code> 안에 2개의 파일이 생성됩니다.</p>

<ul>
  <li><strong>id_rsa</strong> : 생성된 개인키(private key)입니다.</li>
  <li><strong>id_rsa.pub</strong> : 생성된 공개키(public key)입니다.</li>
</ul>

<p><img src="../../assets/built/images/iot-hadoop-rsa-rsapub.png" alt="개인키와 공개키" /></p>

<p>먼저 namenode에서 다음의 명령을 이용해 <code class="highlighter-rouge">authorized_keys</code> 파일을 생성합니다. namenode의 공개키를
<code class="highlighter-rouge">authorized_keys</code> 파일에 저장합니다.</p>

<blockquote>
  <p><strong>cp id_rsa.pub authorized_keys</strong></p>
</blockquote>

<p>namenode의 <code class="highlighter-rouge">.ssh</code> 폴더안에 <code class="highlighter-rouge">authorized_keys</code> 파일이 생성됩니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-authorized-keys.png" alt="authorized_keys" /></p>

<p><code class="highlighter-rouge">ssh-copy-id</code> 유틸리티를 이용해서 datanode01 ~ datanode03에서 각자의 공개키를 namenode에 복사합니다.</p>

<blockquote>
  <p><strong>ssh-copy-id root@namenode</strong> (모든 datanode에서 각각 수행해야 합니다.)</p>
</blockquote>

<p><img src="../../assets/built/images/iot-hadoop-key-copy.png" alt="key 복사" /></p>

<p>각 datanode(01~03)에서 namenode로 공개키를 복사하면 namenode의 authorized_keys의 내용은 아래 그림처럼
저장되게 됩니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-result_authorized_keys.png" alt="결과 authorized_keys" /></p>

<p>이제 통합된 key를 각각의 서버에 보내주면 됩니다.</p>

<blockquote>
  <p><strong>scp -rp authorized_keys root@datanode01:~/.ssh/authorized_keys</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp -rp authorized_keys root@datanode02:~/.ssh/authorized_keys</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp -rp authorized_keys root@datanode03:~/.ssh/authorized_keys</strong></p>
</blockquote>

<p>다음의 명령을 이용해서 <code class="highlighter-rouge">각 서버에서 다른 서버</code>로 SSH Connection을 설정합니다. 
정상적으로 Connection이 설정되면 패스워드 없이 ssh 호출이 정상적으로 이루어집니다.</p>

<blockquote>
  <p><strong>ssh datanode01 date</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">방화벽 설정(방화벽 shutdown)</strong></p>

<p>CentOS의 경우 OS를 처음 설치했을 때 기본 포트를 제외한 모든 포트를 방화벽에서 막고 있습니다.</p>

<p>Hadoop은 내부 데몬(NameNode, SecondaryNameNode, DataNode, JobTracker, TaskTracker)간에 통신을 위해 
다양한 포트를 사용합니다. 만약 Hadoop이 사용하는 포트가 막혀 있다면, Hadoop은 구동했더라도 
HDFS 파일 제어나 MapReduce Job이 정상적으로 실행되지 않게 됩니다.</p>

<p>이러한 현상을 피하기 위해서는 사용되는 Hadoop의 포트를 열어줘야 합니다.
(이 부분은 Hadoop의 공식문서를 참조합니다.)</p>

<p>포트를 하나하나 일일이 변경하고 열어주는 일은 상당히 번거로운 작업이고 대부분은 Hadoop이 
사내 내부망으로만 설치가 되어 있기 때문에 다음과 같이 방화벽을 내려주시는 게 가장 간단한 방법입니다.</p>

<blockquote>
  <p><strong>systemctl stop firewalld</strong></p>
</blockquote>

<blockquote>
  <p><strong>systemctl disable firewalld</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">Hadoop 다운로드 및 설치</strong></p>

<p>Apache site에서 Hadoop 2.9.2 버전을 
<a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz" target="_blank">다운로드</a> 합니다.</p>

<p>다운로드 한 binary 압축 파일을 CentOS로 가져옵니다. 이 부분은 <code class="highlighter-rouge">namenode</code>에서만 작업합니다.</p>

<blockquote>
  <p><strong>tar xvf hadoop-2.9.2.tar.gz</strong></p>
</blockquote>

<blockquote>
  <p><strong>mv hadoop-2.9.2 /usr/local/hadoop</strong></p>
</blockquote>

<p>Hadoop에서 사용할 디렉토리를 생성합니다. 디렉토리 생성은 모든 서버에서 수행해야 합니다.</p>

<blockquote>
  <p><strong>mkdir -p /home/hadoop/hdfs/data</strong></p>
</blockquote>

<blockquote>
  <p><strong>mkdir -p /home/hadoop/hdfs/temp</strong></p>
</blockquote>

<blockquote>
  <p><strong>mkdir -p /home/hadoop/hdfs/name</strong></p>
</blockquote>

<p>이제 Hadoop의 환경설정을 진행합니다. 이 환경설정은 namenode에서만 진행하고 설정이 다 끝나면 다른 서버로
배포하게 됩니다.</p>

<p>Hadoop의 환경설정 파일은 <code class="highlighter-rouge">/usr/local/hadoop/etc/hadoop/</code> 안에 위치하고 있습니다.</p>

<p>설정이 끝난 환경설정파일은 <a href="/assets/downloads/hadoop_config.tar.gz" target="_blank">여기</a>에서 다운로드 받을 수 있습니다.</p>

<p><code class="highlighter-rouge">hadoop-env.sh</code> 파일을 vi로 열어서 <code class="highlighter-rouge">JAVA_HOME</code>에 대한 환경변수를 우리 환경에 맞게 수정합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-hadoop-env.sh.png" alt="hadoop-env.sh 설정" /></p>

<p><code class="highlighter-rouge">hadoop-env.sh</code> 파일의 상단에 다음 내용을 추가하여 아래 그림과 같이 환경변수를 설정합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HADOOP_HOME_WARN_SUPPRESS</span><span class="o">=</span>1
<span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/usr/local/hadoop
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">YARN_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
<span class="nb">export </span><span class="nv">YARN_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
</code></pre></div></div>

<p><img src="../../assets/built/images/iot-hadoop-hadoop-env.sh-new.png" alt="hadoop-env.sh 환경변수 설정" /></p>

<p><code class="highlighter-rouge">yarn-env.sh</code> 파일의 상단에도 다음 내용을 추가합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/usr/local/hadoop
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">YARN_HOME</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
<span class="nb">export </span><span class="nv">YARN_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
</code></pre></div></div>

<p><code class="highlighter-rouge">masters</code> 파일이 존재하지 않는데 vi로 만들어서 secondary namenode를 지정해줍니다.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>datanode01
</code></pre></div></div>

<p><code class="highlighter-rouge">slaves</code> 파일을 vi로 열어서 기존 내용을 삭제하고 datanode 들의 서버를 지정해줍니다.
다음의 내용을 입력합니다.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>datanode01
datanode02
datanode03
</code></pre></div></div>

<p><code class="highlighter-rouge">core-site.xml</code> 파일을 열어 property 내용을 적어줍니다.(hive사용을 위한 설정도 들어가 있습니다.)</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hdfs://namenode:9000<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>/home/hadoop/hdfs/temp<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.hive.hosts<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.hive.groups<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.root.hosts<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.root.groups<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">hdfs-site.xml</code> 파일을 열어 다음과 같이 수정합니다.</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>/home/hadoop/hdfs/name<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.datanode.name.dir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>/home/hadoop/hdfs/data<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.webhdfs.enabled<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.namenode.http.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>namenode:50070<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.secondary.http.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>datanode01:50090<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">mapred-site.xml</code> 파일을 수정합니다. 기본적으로는 mapred-site.xml 파일은 없습니다. 
mapred-site.xml.template을 복사하여 만든 후 수정합니다.</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>mapred.job.tracker<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>namenode:54311<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">yarn-site.xml</code> 파일을 수정합니다.</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>namenode<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.resoucemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>namenode:8025<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>namenode:8030<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>namenode:8040<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p>파일 수정이 끝났으면 Slave 들에게 설정된 Hadoop을 배포합니다.</p>

<blockquote>
  <p><strong>cd /usr/local</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp -r hadoop root@datanode01:/usr/local</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp -r hadoop root@datanode02:/usr/local</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp -r hadoop root@datanode03:/usr/local</strong></p>
</blockquote>

<p><code class="highlighter-rouge">/etc/profile</code> 파일을 각 Slave 들에게 배포합니다.</p>

<blockquote>
  <p><strong>scp /etc/profile root@datanode01:/etc/profile</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp /etc/profile root@datanode02:/etc/profile</strong></p>
</blockquote>

<blockquote>
  <p><strong>scp /etc/profile root@datanode03:/etc/profile</strong></p>
</blockquote>

<p>복사가 모두 끝나면 각 서버에서 Java와 Hadoop이 정상적으로 동작하는지 확인합니다.</p>

<blockquote>
  <p><strong>java -version</strong></p>
</blockquote>

<blockquote>
  <p><strong>hadoop version</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">Hadoop 실행</strong></p>

<p>환경 설정 및 설치가 완료되었으니 이제 hadoop 서버를 실행합니다. 우선 실행전에 포맷을 진행합니다.
이 작업은 <code class="highlighter-rouge">namenode</code>에서만 진행합니다.</p>

<blockquote>
  <p><strong>hadoop namenode -format</strong></p>
</blockquote>

<p>포맷이 끝났으면 <code class="highlighter-rouge">~$HADOOP_HOME/sbin/start-all.sh</code> 으로 실행합니다. (중지는 <code class="highlighter-rouge">stop-all.sh</code>)</p>

<p><code class="highlighter-rouge">namenode</code>에서만 실행 하면 <code class="highlighter-rouge">slaves</code>에 등록되어 있는 모든 datanode의 데몬들도 namenode에서 
구동시키므로 구동 후 각 서버에서 jps로 데몬 구동을 확인 하면 됩니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-start-all.sh.png" alt="hadoop 실헹" /></p>

<p>정상적으로 실행되었는지 확인하기 위해 각 서버에서 <code class="highlighter-rouge">jps</code>(JVM 위에서 돌아가는 프로세스 확인) 명령을 수행합니다.</p>

<blockquote>
  <p><strong>jps</strong></p>
</blockquote>

<p>master에는 <code class="highlighter-rouge">ResourceManager</code>가 구동 되어야 하고 slave들에는 <code class="highlighter-rouge">NodeManager</code>가 구동 되어야 클러스터가 정상 동작합니다.</p>

<p>다음의 URL을 이용하면 HDFS를 확인할 수 있습니다. Live Nodes : 3 ( 3개의 slave 존재 확인 )</p>

<blockquote>
  <p><strong>localhost:50070</strong></p>
</blockquote>

<p><img src="../../assets/built/images/iot-hadoop-status.png" alt="hadoop 실헹상태 확인" /></p>

<p>간단한 명령어를 이용해 파일 시스템을 다룰 수 있습니다.</p>

<ul>
  <li>디렉토리 생성</li>
</ul>

<blockquote>
  <p><strong>hadoop fs -mkdir /test</strong></p>
</blockquote>

<ul>
  <li>파일 업로드</li>
</ul>

<blockquote>
  <p><strong>hadoop fs -put (업로드 할 파일)  (타겟 디렉토리)</strong></p>
</blockquote>

<ul>
  <li>디렉토리 리스트 출력</li>
</ul>

<blockquote>
  <p><strong>hadoop fs -ls (타겟 디렉토리)</strong></p>
</blockquote>

<p>참고로 Hadoop이 비정상 종료 시 <code class="highlighter-rouge">safe</code> 모드로 전환되는데 이 경우 에러가 발생합니다.
이런 경우 다음의 명령을 이용해서 Safe mode를 OFF해야 합니다.</p>

<blockquote>
  <p><strong>hdfs dfsadmin -safemode leave</strong></p>
</blockquote>

<p>현재 모드가 Safe 모드인지 확인 하는 방법은 다음의 명령을 이용하면 됩니다.</p>

<blockquote>
  <p><strong>hdfs dfsadmin -safemode get</strong></p>
</blockquote>

<p>마지막으로 파일시스템을 검사할 수 있는데 다음의 명령을 이용하면 됩니다.</p>

<blockquote>
  <p><strong>hdfs fsck /</strong></p>
</blockquote>

<hr />

<p><strong class="subtitle_fontAwesome">Hadoop Example</strong></p>

<p><strong class="subtitle2_fontAwesome">Hadoop File 처리</strong></p>

<p>가장 기본적인 예제인 <code class="highlighter-rouge">File Read Write</code>를 통해서 Hadoop 사용법을 알아보겠습니다.</p>

<p>기본적으로 Hadoop 프로그래밍은 Maven 빌드를 사용하는데 이번에는 Maven을 사용하지 않고 직접 jar file을 다운로드 받아서
처리하겠습니다. 필요한 jar파일은 <code class="highlighter-rouge">hadoop-core jar</code>파일입니다. 다운받기 위해서는 <code class="highlighter-rouge">Maven Repository</code>에 가야합니다.</p>

<p>Maven Repository에 가서 검색창에 <code class="highlighter-rouge">hadoop core</code>를 입력하고 검색하면 <code class="highlighter-rouge">hadoop-core-1.2.1.jar</code> 파일을 다운로드 받을 수 있습니다.
이 파일을 우리 Eclipse project의 적당한 곳에 복사한 후 Build Path 설정에서 라이브러리로 등록합니다.</p>

<p>다음의 코드를 작성합니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Exam01_SimpleFileReadWrite</span> <span class="o">{</span>

	<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>

            <span class="k">if</span> <span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="na">length</span> <span class="o">!=</span> <span class="mi">2</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Usage : Exam01_SimpleFileReadWrite &lt;filename&gt; &lt;contents&gt;"</span><span class="o">);</span>
                <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">();</span>
            <span class="o">}</span>
            <span class="n">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
            <span class="c1">// Configuration class는 Hadoop의 설정값을 가져온다. </span>
            <span class="c1">// Hadoop xml 설정값들을 확인.</span>
            <span class="k">try</span> <span class="o">{</span>
                <span class="n">FileSystem</span> <span class="n">hdfs</span> <span class="o">=</span> <span class="n">FileSystem</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>
                <span class="c1">// FileSystem은 Hadoop의 FileSystem. </span>
                <span class="c1">// Hadoop의 설정값을 보고 파일시스템을 가져온다.</span>
                <span class="n">Path</span> <span class="n">path</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">]);</span>
                <span class="k">if</span> <span class="o">(</span><span class="n">hdfs</span><span class="o">.</span><span class="na">exists</span><span class="o">(</span><span class="n">path</span><span class="o">))</span> <span class="o">{</span>
                    <span class="n">hdfs</span><span class="o">.</span><span class="na">delete</span><span class="o">(</span><span class="n">path</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
                <span class="o">}</span>
                <span class="c1">// 파일 시스템을 가져온후 출력 스트림을 열어서 args[0]이름으로 파일을 생성한다. </span>
                <span class="c1">// 그후 args[1]을 파일 내용으로 저장한다.</span>
                <span class="c1">// 그 후 다시 입력 스트림을 열어서 HDFS에서 해당 파일을 가져온 후 그 내용을 출력.</span>
                <span class="n">FSDataOutputStream</span> <span class="n">outStream</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">path</span><span class="o">);</span>
                <span class="n">outStream</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]);</span>
                <span class="n">outStream</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
    
                <span class="n">FSDataInputStream</span> <span class="n">inputStream</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="na">open</span><span class="o">(</span><span class="n">path</span><span class="o">);</span>
                <span class="n">String</span> <span class="n">inputString</span> <span class="o">=</span> <span class="n">inputStream</span><span class="o">.</span><span class="na">readUTF</span><span class="o">();</span>
                <span class="n">inputStream</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
                <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"읽은 내용 : "</span> <span class="o">+</span> <span class="n">inputString</span><span class="o">);</span>
            <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
            <span class="o">}</span>
	<span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>이제 우리 Project를 <code class="highlighter-rouge">jar</code> 파일로 export 시킵니다. 해당 jar파일을 <code class="highlighter-rouge">yarn</code>을 이용해서 실행하는 과정을 거쳐야 
합니다. <code class="highlighter-rouge">yarn</code>은 작업을 실행해주는 모듈입니다.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yarn (실행시킬 타입) (실행시킬 파일 이름) (실행시킬 클래스경로) [-옵션]
</code></pre></div></div>

<blockquote>
  <p><strong>yarn jar FileReadWriteExam.jar javaHadoop.Exam01_SimpleFileReadWrite test.txt HelloWorld</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">MapReduce</strong></p>

<p>이번에는 Hadoop의 데이터 처리 기술인 <code class="highlighter-rouge">MapReduce</code>에 대해서 알아보겠습니다. 사실 MapReduce개념은 Hadoop의 고유개념은
아니고 데이터 처리 솔루션들은 대부분 가지고 있는 기술입니다.</p>

<p>MapReduce는 <code class="highlighter-rouge">Map</code>과 <code class="highlighter-rouge">Reduce</code> 두 개의 기능으로 나뉘며 Map은 특정 데이터를 가져와서 Key와 Value의 쌍으로 만드는
일을 합니다. Reduce는 Map에서 묶은 Key와 Value을 이용해서 내가 필요한 정보로 다시 Key와 Value의 쌍으로
데이터를 조작하는 것을 의미합니다.</p>

<p>여기서는 <code class="highlighter-rouge">WordCount</code>라는 아주 유명한 입문 예제를 가지고 MapReduce를 이해해 보도록 하겠습니다.</p>

<p>MapReduce작업을 위해서는 반드시 3가지의 클래스가 필요합니다. 이는 각각 <code class="highlighter-rouge">Mapper</code>와 <code class="highlighter-rouge">Reducer</code> 그리고 <code class="highlighter-rouge">Driver</code>입니다.
Driver는 자바의 entry point인 main method가 포함된 class를 지칭합니다.</p>

<p>먼저 Mapper class를 살펴보겠습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">WordCountMapper</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">final</span> <span class="kd">static</span> <span class="n">IntWritable</span> <span class="n">one</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntWritable</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
    <span class="kd">private</span> <span class="n">Text</span> <span class="n">word</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="o">();</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">LongWritable</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> 
    		<span class="n">Mapper</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;.</span><span class="na">Context</span> <span class="n">context</span><span class="o">)</span>
            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
        <span class="n">StringTokenizer</span> <span class="n">itr</span> <span class="o">=</span> <span class="k">new</span> <span class="n">StringTokenizer</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="na">hasMoreTokens</span><span class="o">())</span> <span class="o">{</span>
            <span class="n">word</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="na">nextToken</span><span class="o">());</span>
            <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="n">one</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Mapper class는 4개의 인자를 받습니다.</p>

<p>MapReduce에서의 인자는 일반 long형이나 int형, String형을 쓰지 않습니다. 대신 
long이나 int형은 각각 <code class="highlighter-rouge">LongWritable</code>, <code class="highlighter-rouge">IntWritable</code>이라는 Hadoop 전용 자료형을 사용합니다.
그리고 String은 <code class="highlighter-rouge">Text</code>로 사용합니다.</p>

<p>Mapper class의 인자는 순서대로 Input Key, Input Value, Output Key, Output Value를 의미합니다.
Input으로 들어오는것 역시 Key와 Value쌍이고 Output 역시 Key와 Value쌍 입니다.</p>

<p>일반적으로 Input의 경우 Key는 <code class="highlighter-rouge">LongWritable</code>이고 Value는 <code class="highlighter-rouge">Text</code>로 고정되는 경향이 있습니다.
원래 Input에는 Key값이 있을리가 없습니다. 그런데 값이 들어올때 Hadoop은 개행 단위로 끊어서 값을 받아오고 
그 순서대로 번호를 매기게 됩니다. 그리고 그 번호의 값이 Key가 됩니다. 
따라서 Key값은 숫자가 되고 빅데이터 처리에서 데이터량이 많을 수 있으므로 LongWritable로 사용하는 
것입니다. 받는 값을 Text로 하는 이유는 가공의 편의성 때문입니다.</p>

<p>따라서 Input의 경우 Key는 LongWritable이되고 Value는 Text가 됩니다.</p>

<p>반면에 Output의 경우는 Key와 Value는 임의로 지정할 수 있습니다.위의 예제에서는 
StringTokenizer로 공백단위로 들어온 텍스트를 끊어내고 그 값을 Output Key로 정하고 
그 Value는 1로 고정해서 출력합니다.</p>

<p>Reducer로 Key,Value 쌍을 보내는 작업은 <code class="highlighter-rouge">context.write</code>이 담당합니다.</p>

<p>정리하자면, <code class="highlighter-rouge">&lt;들어온순서, 라인단위 텍스트&gt;</code>로 Input값이 들어오게 되고 반대로 
Output은 <code class="highlighter-rouge">&lt;끊은 단어,1&gt;</code>값을 출력하게 됩니다.</p>

<p>이번에는 Reducer class를 살펴보겠습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">WordCountReducer</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="n">IntWritable</span> <span class="n">result</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntWritable</span><span class="o">();</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Text</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">IntWritable</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span>
                          <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;.</span><span class="na">Context</span> <span class="n">context</span><span class="o">)</span> 
                            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
        <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">IntWritable</span> <span class="n">val</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
        <span class="o">}</span>
        <span class="n">result</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">sum</span><span class="o">);</span>
        <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">result</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Reduce 작업은 사실 Map 로직과 거의 동일합니다. Reducer는 Mapper의 Output을 가지고 작업을 하게 되며
Reducer도 총 4개의 인자를 가지는데 Mapper와 동일합니다. 즉, Input Key, Input Value, Output Key, Output Value 입니다.
당연히 Mapper의 Output 인자와 Reducer의 Input 인자는 같아야합니다.</p>

<p>한가지 특이한점은 Reducer의 Value는 Mapper와는 다르게 복수형으로 일종의 컬렉션을 받게된다는 것입니다. 
그 이유는 Key에 중복이 존재할 수 있기 때문입니다.</p>

<p>Reducer 작업이 Mapper와 다른점이 있다면 MApper는 순서대로 값을 받으므로 Input Key는 중복될 수 없습니다.
그러나 Reducer는 Mapper의 Output을 받게되는데 Mapper의 Output Key는 중복이 안된다는 제약은 없습니다.
따라서 Reducer의 Input Key는 중복될 수 있습니다.</p>

<p>Reducer의 실질적은 역활은 <code class="highlighter-rouge">중복된 키들을 어떻게 처리하느냐</code> 입니다.
Mapper는 의미가 같다고 생각되는 데이터를 같은 Key로 만들어서 출력합니다. 그러면 
Reducer에서는 그 동일한 Key를 묶어서 처리를 한다고 보면 됩니다.</p>

<p>위의 예제에서 Value들은 모두 1로 고정되어있습니다. 같은 Key의 경우에 계속해서 더하게 됩니다. 
즉, 들어가는 데이터에 hello라는 단어가 10개가 들어왔다고 가정해 보면 &lt;hello,1&gt;이라는 Output이 
Mapper에 의해 생성될 것이고 Reducer에서는 저 hello라는 Key가 10개가 있으므로 10번 for문을 돌면서 
합을 계산해 10이라는 값을 계산하게 됩니다.</p>

<p>마지막으로 Driver class를 살펴보겠습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Exam02_WordCountMapReduce</span> <span class="o">{</span>

	<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>

        <span class="n">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="na">length</span> <span class="o">!=</span> <span class="mi">2</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Usage: Exam02_WordCountMapReduce &lt;input&gt; &lt;output&gt;"</span><span class="o">);</span>
            <span class="n">System</span><span class="o">.</span><span class="na">exit</span><span class="o">(</span><span class="mi">2</span><span class="o">);</span>
            <span class="o">;</span>
        <span class="o">}</span>
        <span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Job</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="s">"WordCount"</span><span class="o">);</span>
        <span class="n">job</span><span class="o">.</span><span class="na">setJarByClass</span><span class="o">(</span><span class="n">Exam02_WordCountMapReduce</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
        <span class="n">job</span><span class="o">.</span><span class="na">setMapperClass</span><span class="o">(</span><span class="n">WordCountMapper</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
        <span class="n">job</span><span class="o">.</span><span class="na">setReducerClass</span><span class="o">(</span><span class="n">WordCountReducer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

        <span class="n">job</span><span class="o">.</span><span class="na">setInputFormatClass</span><span class="o">(</span><span class="n">TextInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
        <span class="n">job</span><span class="o">.</span><span class="na">setOutputFormatClass</span><span class="o">(</span><span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

        <span class="n">job</span><span class="o">.</span><span class="na">setOutputKeyClass</span><span class="o">(</span><span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
        <span class="n">job</span><span class="o">.</span><span class="na">setOutputValueClass</span><span class="o">(</span><span class="n">IntWritable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

        <span class="n">FileInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">]));</span>
        <span class="n">FileOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]));</span>
        <span class="n">job</span><span class="o">.</span><span class="na">waitForCompletion</span><span class="o">(</span><span class="kc">true</span><span class="o">);</span>
    <span class="o">}</span>	
<span class="o">}</span>
</code></pre></div></div>

<p>Driver로 Mapper와 Reducer를 등록하고 실행하면 됩니다.</p>

<p>이제 실행을 하기 위해서 샘플로 사용할 입력 파일을 하나 생성합니다. (wow_wordcount.txt)</p>

<p>생성한 txt 파일을 Hadoop 파일 시스템에 저장합니다.</p>

<blockquote>
  <p><strong>hdfs dfs -put wow_wordcount.txt</strong></p>
</blockquote>

<p>다음의 명령을 이용해서 파일 시스템에 정상적으로 저장되었는지를 확인합니다.</p>

<blockquote>
  <p><strong>hdfs dfs -ls</strong></p>
</blockquote>

<p>작성된 프로그램(Java Project)을 jar파일로 생성한 후 yarn으로 실행시킵니다.
output 결과는 텍스트가 아니라 폴더로 나오기에 확장자를 붙혀줄 필요는 없습니다.</p>

<blockquote>
  <p><strong>yarn jar wordCounterExam.jar javaHadoop.Exam02_WordCountMapReduce wow_wordcount.txt output</strong></p>
</blockquote>

<blockquote>
  <p><strong>hdfs dfs -ls output</strong></p>
</blockquote>

<p>output 디렉토리 안에 파일은 <code class="highlighter-rouge">part-r-00000</code>로 저장됩니다. 이 파일은 텍스트 파일이므로 cat으로 읽을 수 있습니다.
다음의 명령을 이용하여 결과를 확인합니다.</p>

<blockquote>
  <p><strong>hdfs dfs -cat output/part-r-00000</strong></p>
</blockquote>

<hr />

<p><strong class="subtitle_fontAwesome">Hive 설치 및 활용</strong></p>

<p>현재 서비스 로그 및 사용자 생성 데이터는 점점 크고 빠르게 늘어나고 있습니다. 
우리가 설치한 Hadoop은 이런 매우 큰 데이터를 저장하고 처리하기 적합한 가장 인기 있는 오픈 소스입니다.</p>

<p>하지만 데이터 분석을 위해 <code class="highlighter-rouge">MapReduce</code>를 직접 구현하기에는 코드가 매우 어려운 형태이며, 
따라서 시스템을 구현하기 위해서는 그만한 개발 능력이 되는 개발자가 있어야 하는 등 실제 시스템을 도입해서 사용하는데는
어려움도 있는게 사실입니다.</p>

<p>그런 이유로 <code class="highlighter-rouge">MapReduce</code> 처리를 쉽게 해주기 위한 대안으로 <code class="highlighter-rouge">Hive</code>를 사용합니다. (다른 framework도 존재합니다.) 
Hive는 <code class="highlighter-rouge">RDB</code>의 SQL문을 작성해 본 개발자라면 상당히 익숙한 형태로 데이터 분석 작업을 진행할 수 있습니다.</p>

<p>정리하자면, <code class="highlighter-rouge">Hive</code>는 Hadoop 상의 오픈소스 <code class="highlighter-rouge">Data Warehouse</code> 솔루션입니다.
DW(Data Warehouse)는 리포팅 및 분석을 위한 Database라고 생각하면 됩니다.</p>

<p>간단하게 Hive의 동작방법에 대해 알아보면 다음과 같습니다.</p>

<p>Hive는 Hadoop상에서 동작합니다. 당연히 데이터 또한 HDFS에 저장되어 있습니다.
Hive는 메타스토어(<code class="highlighter-rouge">Metastore</code>)라는 저장소를 만들어 Hadoop에서 처리된 메타데이터의 구조를 메타스토어에 저장합니다.
하이브는 Oracle, MySQL 등 JDBC를 지원하는 모든 데이터베이스를 이용해 메타스토어를 구축할 수 있습니다.
디폴트로 <code class="highlighter-rouge">Apache Derby</code>를 사용하지만, 일반적으로 MySQL이나 Postgres를 많이 사용합니다.</p>

<p><img src="../../assets/built/images/hive-hadoop-hive-architecture.png" alt="Hadoop Hive 구조" /></p>

<ol>
  <li>사용자가 제출한 SQL문을 <code class="highlighter-rouge">Driver</code>가 Compiler에 요청하여 MetaStore의 정보를 이용해 처리에 적합한 형태로 컴파일</li>
  <li>컴파일된 SQL을 실행엔진으로 실행</li>
  <li>리소스 매니저가 클러스터의 자원을 적절히 활용하여 실행</li>
  <li>실행 중 사용하는 원천데이터는 HDFS등의 저장장치를 이용</li>
  <li>실행결과를 사용자에게 반환</li>
</ol>

<p><strong class="subtitle2_fontAwesome">MySQL 설치</strong></p>

<p>MySQL은 Hadoop namenode에 설치하겠습니다.</p>

<p>먼저 wget을 이용해서 MySQL을 설치합니다.</p>

<blockquote>
  <p><strong>wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</strong></p>
</blockquote>

<blockquote>
  <p><strong>rpm -ivh mysql-community-release-el7-5.noarch.rpm</strong></p>
</blockquote>

<blockquote>
  <p><strong>yum install mysql-server</strong></p>
</blockquote>

<blockquote>
  <p><strong>systemctl start mysqld</strong></p>
</blockquote>

<blockquote>
  <p><strong>systemctl enable mysqld</strong></p>
</blockquote>

<blockquote>
  <p><strong>systemctl status mysqld</strong></p>
</blockquote>

<p><code class="highlighter-rouge">mysql_secure_installation</code> 명령으로 기본적인 보안 설정을 합니다. 몇가지 설정을 수행합니다.</p>

<p><code class="highlighter-rouge">root</code> 권한으로 mysql console에 진입합니다.</p>

<blockquote>
  <p><strong>mysql -u root -p</strong></p>
</blockquote>

<p>일단 mysql 버전부터 확인하고 진행하겠습니다.</p>

<blockquote>
  <p><strong>SHOW VARIABLES LIKE “%version%”;</strong></p>
</blockquote>

<p><img src="../../assets/built/images/iot-hadoop-mysql-version.png" alt="MySQL Version 확인" /></p>

<p>이제 사용자 계정을 생성합니다.</p>

<blockquote>
  <p><strong>CREATE USER hive IDENTIFIED BY “hive”;</strong></p>
</blockquote>

<blockquote>
  <p><strong>CREATE USER hive@localhost IDENTIFIED BY “hive”;</strong></p>
</blockquote>

<p>이제 사용할 database를 만들고 hive 계정에 해당 database의 권한을 부여합니다.</p>

<blockquote>
  <p><strong>CREATE DATABASE hivedb;</strong></p>
</blockquote>

<blockquote>
  <p><strong>GRANT ALL PRIVILEGES ON hivedb.* TO hive;</strong></p>
</blockquote>

<blockquote>
  <p><strong>GRANT ALL PRIVILEGES ON hivedb.* TO hive@localhost;</strong></p>
</blockquote>

<p>설정된 권한을 바로 적용하고 mysql console을 종료합니다.</p>

<blockquote>
  <p><strong>FLUSH PRIVILEGES;</strong></p>
</blockquote>

<blockquote>
  <p><strong>EXIT</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">참고 : MySQL 삭제</strong></p>

<p>기존 설치되어 있는 MySQL 패키지를 제거하려면 다음과 같은 명령을 사용해야 합니다.</p>

<blockquote>
  <p><strong>yum remove mysql mysql-server</strong></p>
</blockquote>

<p>패키지를 삭제한 후 남아있는 MySQL 디렉토리를 삭제합니다.</p>

<blockquote>
  <p><strong>rm -rf /var/lib/mysql</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">HIVE 설치</strong></p>

<p><code class="highlighter-rouge">http://hive.apache.org</code>에서 2.3.7 버젼을 다운로드 받아 압축을 풀어줍니다.</p>

<blockquote>
  <p><strong>wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.7/apache-hive-2.3.7-bin.tar.gz</strong></p>
</blockquote>

<blockquote>
  <p><strong>tar -zxvf apache-hive-2.3.7-bin.tar.gz</strong></p>
</blockquote>

<blockquote>
  <p><strong>mv apache-hive-2.3.7-bin /usr/local/hive</strong></p>
</blockquote>

<p><code class="highlighter-rouge">/etc/profile</code>을 vi로 열어서 아래의 내용을 추가합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HIVE_HOME</span><span class="o">=</span>/usr/local/hive
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$HIVE_HOME</span>/bin:<span class="nv">$PATH</span>
</code></pre></div></div>

<p>위에서 설치한 MySQL과 연동하기 위해서 MySQL JDBC Connector를 설치합니다.</p>

<blockquote>
  <p><strong>wget http://www.java2s.com/Code/JarDownload/mysql/mysql-connector-java-commercial-5.1.7-bin.jar.zip</strong></p>
</blockquote>

<blockquote>
  <p><strong>unzip mysql-connector-java-commercial-5.1.7-bin.jar.zip</strong></p>
</blockquote>

<blockquote>
  <p><strong>cp mysql-connector-java-commercial-5.1.7-bin.jar  $HIVE_HOME/lib/</strong></p>
</blockquote>

<p>다음은 사용할 HDFS 디렉토리를 생성해야 합니다. 다음의 명령을 이용하여 HDFS 디렉토리를 생성합니다. 당연히 Hadoop이 
실행되고 있는 상황이어야 합니다.</p>

<blockquote>
  <p><strong>hadoop fs -mkdir /tmp</strong></p>
</blockquote>

<blockquote>
  <p><strong>hadoop fs -mkdir -p /user/hive/warehouse</strong></p>
</blockquote>

<blockquote>
  <p><strong>hadoop fs -chmod g+w /tmp</strong></p>
</blockquote>

<blockquote>
  <p><strong>hadoop fs -chmod g+w /user/hive/warehouse</strong></p>
</blockquote>

<p>$HIVE_HOME/conf안의 hive-env.sh.template를 hive-env.sh로 복사한 후에 HADOOP_HOME을 아래와 같이 수정합니다.</p>

<blockquote>
  <p><strong>mv $HIVE_HOME/conf/hive-env.sh.template $HIVE_HOME/conf/hive-env.sh</strong></p>
</blockquote>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">HADOOP_HOME</span><span class="o">=</span>/usr/local/hadoop
</code></pre></div></div>

<p>$HIVE_HOME/conf안에 hive-site.xml 파일을 생성하여 생성된 내용을 입력합니다.
<code class="highlighter-rouge">UserName</code>과 <code class="highlighter-rouge">Password</code>는 MySQL에서 사용하게될 UserName과 Password입니다.</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span>
<span class="cp">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span>
<span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.metastore.local<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.metastore.warehouse.dir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hdfs://namenode:9000/user/hive/warehouse<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionUserName<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hive<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionPassword<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hive<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.metastore.schema.verification<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.server2.thrift.port<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>10000<span class="nt">&lt;/value&gt;</span>
                <span class="nt">&lt;description&gt;</span>TCP port number to listen on, default 10000<span class="nt">&lt;/description&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.server2.enable.doAs<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.server2.authentication<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>NONE<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.metastore.sasl.enabled<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
                <span class="nt">&lt;description&gt;</span>If true, the metastore Thrift interface will be secured with SASL.
                        Clients must authenticate with Kerberos.<span class="nt">&lt;/description&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.server2.enable.impersonation<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;description&gt;</span>Enable user impersonation for HiveServer2<span class="nt">&lt;/description&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.server2.thrift.bind.host<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>192.168.64.128<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.support.concurrency<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.enforce.bucketing<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.exec.dynamic.partition.mode<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>nonstrict<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.txn.manager<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.compactor.initiator.on<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hive.compactor.worker.threads<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div></div>

<p><strong class="subtitle2_fontAwesome">HIVE 실행</strong></p>

<p>Hive 2.1부터는 초기화 단계로 아래의 코드를 실행해야 합니다.
실행한 뒤 <code class="highlighter-rouge">schemaTool completed</code>가 출력되면 됩니다.</p>

<blockquote>
  <p><strong>schematool -dbType mysql -initSchema</strong></p>
</blockquote>

<p><img src="../../assets/built/images/iot-hadoop-hive-init.png" alt="HIVE 초기화" /></p>

<p>초기화가 정상적으로 진행되면 <code class="highlighter-rouge">hive</code> 명령을 실행합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-hive-run.png" alt="HIVE 실행" /></p>

<p>정상적으로 동작하는지 확인하기 위해 Hive QL을 이용하여 Table을 생성하고 조회합니다.</p>

<blockquote>
  <p><strong>CREATE TABLE mytbl(seq INT, contents STRING);</strong></p>
</blockquote>

<blockquote>
  <p><strong>SELECT * FROM mysql</strong></p>
</blockquote>

<p><img src="../../assets/built/images/iot-hadoop-hive-create-table.png" alt="HIVE CREATE Table" /></p>

<p>MySQL Server에 접속해서 생성된 Table이 존재하는지 확인합니다.</p>

<blockquote>
  <p><strong>show databases;</strong></p>
</blockquote>

<blockquote>
  <p><strong>show tables;</strong></p>
</blockquote>

<blockquote>
  <p><strong>SELECT * FROM hivedb.TBLS;</strong></p>
</blockquote>

<p><strong class="subtitle2_fontAwesome">HIVE java 연동</strong></p>

<p>먼저 Host PC의 Eclipse에서 작성된 Java 프로그램이 VMware로 생성한 Hadoop01 가상머신에 내부 IP를 이용하여
접속이 가능하도록 VMware의 <code class="highlighter-rouge">Edit &gt; Virtual Network Editor</code>를 선택합니다.</p>

<p>설정을 바꾸기 위해 하단의 <code class="highlighter-rouge">change settings</code>를 클릭합니다.</p>

<p>아래의 그림과 같이 <code class="highlighter-rouge">VMnet8</code>을 선택한 후 <code class="highlighter-rouge">connect a host virtual adapter to this network</code> 체크박스를 체크합니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-hive-network.png" alt="네트워크 설정" /></p>

<p>Host PC의 네트워크 설정을 살펴보면 다음과 같이 표현됩니다.</p>

<p><img src="../../assets/built/images/iot-hadoop-hive-host-network.png" alt="네트워크 설정" /></p>

<p>자바 프로그램에서 Hive에 접근할 수 있게 Hive를 기동합니다.</p>

<blockquote>
  <p><strong>hiveserver2</strong></p>
</blockquote>

<p>Host PC의 Eclipse에서 Maven Project를 생성합니다.</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;project</span> <span class="na">xmlns=</span><span class="s">"http://maven.apache.org/POM/4.0.0"</span> <span class="na">xmlns:xsi=</span><span class="s">"http://www.w3.org/2001/XMLSchema-instance"</span>
  <span class="na">xsi:schemaLocation=</span><span class="s">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;modelVersion&gt;</span>4.0.0<span class="nt">&lt;/modelVersion&gt;</span>

  <span class="nt">&lt;groupId&gt;</span>test.bbb<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>SampleTest<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>0.0.1-SNAPSHOT<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;packaging&gt;</span>jar<span class="nt">&lt;/packaging&gt;</span>

  <span class="nt">&lt;name&gt;</span>SampleTest<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;url&gt;</span>http://maven.apache.org<span class="nt">&lt;/url&gt;</span>

  <span class="nt">&lt;properties&gt;</span>
    <span class="nt">&lt;project.build.sourceEncoding&gt;</span>UTF-8<span class="nt">&lt;/project.build.sourceEncoding&gt;</span>
  <span class="nt">&lt;/properties&gt;</span>

  <span class="nt">&lt;dependencies&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>junit<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>junit<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>3.8.1<span class="nt">&lt;/version&gt;</span>
      <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
            <span class="nt">&lt;groupId&gt;</span>org.apache.hive<span class="nt">&lt;/groupId&gt;</span>
            <span class="nt">&lt;artifactId&gt;</span>hive-jdbc<span class="nt">&lt;/artifactId&gt;</span>
            <span class="nt">&lt;version&gt;</span>0.12.0<span class="nt">&lt;/version&gt;</span>
	<span class="nt">&lt;/dependency&gt;</span>
	<span class="nt">&lt;dependency&gt;</span>
            <span class="nt">&lt;groupId&gt;</span>jdk.tools<span class="nt">&lt;/groupId&gt;</span>
            <span class="nt">&lt;artifactId&gt;</span>jdk.tools<span class="nt">&lt;/artifactId&gt;</span>
            <span class="nt">&lt;version&gt;</span>1.8.0_221<span class="nt">&lt;/version&gt;</span>	
            <span class="nt">&lt;scope&gt;</span>system<span class="nt">&lt;/scope&gt;</span>
            <span class="nt">&lt;systemPath&gt;</span>C:/Program Files/Java/jdk1.8.0_221/lib/tools.jar<span class="nt">&lt;/systemPath&gt;</span>	
	<span class="nt">&lt;/dependency&gt;</span>
	<span class="nt">&lt;dependency&gt;</span>                                                                                                                                       
    	<span class="nt">&lt;groupId&gt;</span>org.apache.hadoop<span class="nt">&lt;/groupId&gt;</span>                                                                                                           
    	<span class="nt">&lt;artifactId&gt;</span>hadoop-common<span class="nt">&lt;/artifactId&gt;</span>                                                                                                         
    	<span class="nt">&lt;version&gt;</span>3.2.0<span class="nt">&lt;/version&gt;</span>                                                                                            
	<span class="nt">&lt;/dependency&gt;</span>  
  <span class="nt">&lt;/dependencies&gt;</span>
<span class="nt">&lt;/project&gt;</span> 
</code></pre></div></div>

<p>일반적은 JDBC 코드를 작성합니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">App</span> <span class="o">{</span>
	<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">Connection</span> <span class="n">conn</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
        <span class="n">ResultSet</span> <span class="n">rs</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
       
        <span class="k">try</span> <span class="o">{</span>           
            <span class="n">String</span> <span class="n">driver</span> <span class="o">=</span> <span class="s">"org.apache.hive.jdbc.HiveDriver"</span><span class="o">;</span>
            <span class="n">Class</span><span class="o">.</span><span class="na">forName</span><span class="o">(</span><span class="n">driver</span><span class="o">);</span>
           
            <span class="n">String</span> <span class="n">url</span> <span class="o">=</span> <span class="s">"jdbc:hive2://192.168.64.128:10000/hivedb"</span><span class="o">;</span>
            <span class="n">String</span> <span class="n">id</span> <span class="o">=</span> <span class="s">"hive"</span><span class="o">;</span>
            <span class="n">String</span> <span class="n">pw</span> <span class="o">=</span> <span class="s">"hive"</span><span class="o">;</span>
           
            <span class="n">conn</span> <span class="o">=</span> <span class="n">DriverManager</span><span class="o">.</span><span class="na">getConnection</span><span class="o">(</span><span class="n">url</span><span class="o">,</span> <span class="n">id</span><span class="o">,</span> <span class="n">pw</span><span class="o">);</span>
           
            <span class="n">String</span> <span class="n">sql</span> <span class="o">=</span> <span class="s">"SELECT count(*) FROM mytbl"</span><span class="o">;</span>
            <span class="n">Statement</span> <span class="n">stmt</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="na">createStatement</span><span class="o">();</span>
            <span class="n">rs</span> <span class="o">=</span> <span class="n">stmt</span><span class="o">.</span><span class="na">executeQuery</span><span class="o">(</span><span class="n">sql</span><span class="o">);</span>
           
            <span class="k">while</span><span class="o">(</span><span class="n">rs</span><span class="o">.</span><span class="na">next</span><span class="o">()){</span>
                <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="na">getInt</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>                       
                <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span> <span class="s">"결과 : "</span> <span class="o">+</span> <span class="n">col</span><span class="o">);</span>
            <span class="o">}</span>
           
            <span class="n">rs</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
            <span class="n">conn</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
           
        <span class="o">}</span> <span class="k">catch</span><span class="o">(</span><span class="n">Exception</span> <span class="n">ex</span><span class="o">){</span>
            <span class="n">ex</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
        <span class="o">}</span> <span class="k">finally</span> <span class="o">{</span>
            <span class="k">try</span><span class="o">{</span>
                <span class="k">if</span><span class="o">(</span> <span class="n">rs</span> <span class="o">!=</span> <span class="kc">null</span> <span class="o">){</span>
                    <span class="n">rs</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>               
                <span class="o">}</span>
                <span class="k">if</span><span class="o">(</span> <span class="n">conn</span> <span class="o">!=</span> <span class="kc">null</span> <span class="o">){</span>
                    <span class="n">conn</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>               
                <span class="o">}</span>                
            <span class="o">}</span> <span class="k">catch</span><span class="o">(</span><span class="n">Exception</span> <span class="n">ex</span><span class="o">){</span>
                <span class="n">rs</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
                <span class="n">conn</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>End.</p>

<hr />

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            <!-- 기존에 있던 post하단 이메일 subscribe 삭제 -->

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/built/images/economy-author-logo.jpg" alt="moon9342" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/moon9342">moon9342</a></h4>
                                
                                    <p><a href="https://www.youtube.com/channel/UCp-MztINXTRVkRGCnqnYNlQ">얼큰우동TV, 쉽게배우는 IT(Programming,Machine Learning,금융)</a></p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/moon9342">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover2.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; IT Technical Training &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/et-cetera/">Et-cetera</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/arduino">Arduino</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/can">CAN Protocol</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/gulp-css-compile">Gulp CSS Compile 문제 해결</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/et-cetera/">
                                
                                    See all 18 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                
    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/can">
                <div class="post-card-image" style="background-image: url(/assets/built/images/iot-logo.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/can">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Et-cetera</span>
                            
                        
                    

                    <h2 class="post-card-title">CAN Protocol</h2>
                </header>
                <section class="post-card-excerpt">
                    <p>**{% include subtitle-fontawesome.html %} CAN 개요** `CAN`은 Controller Area Network의 약자로 1986년에 독일의 메르세데스 벤츠사의 요구 (자동차내의 3개의 서로다른 ECU간의 데이터 통신)로 자동차 부품회사인 로베르트 보쉬사가 개발해 자동차기술자 협회에서 제안한 네트워크 시스템입니다. `ECU`는</p>
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/built/images/economy-author-logo.jpg" alt="moon9342" />
                        
                        <span class="post-card-author">
                            <a href="/author/moon9342/">moon9342</a>
                        </span>
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                
    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/R-lecture-R-database">
                <div class="post-card-image" style="background-image: url(/assets/built/images/da-logo.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/R-lecture-R-database">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">R</span>
                            
                        
                    

                    <h2 class="post-card-title">R 기초강의(22) - R 정형데이터 처리</h2>
                </header>
                <section class="post-card-excerpt">
                    <p>R 강좌는 여러 절로 구성되어 있습니다. R 기초강의(1) - R 개요 R 기초강의(2) - R Operator R 기초강의(3) - R Data Type R 기초강의(4) - R 패키지와 함수 R 기초강의(5) - R 자료구조(vector)</p>
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/built/images/economy-author-logo.jpg" alt="moon9342" />
                        
                        <span class="post-card-author">
                            <a href="/author/moon9342/">moon9342</a>
                        </span>
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
            <span>IT Technical Training</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Hadoop 설치와 활용</div>



    <div class="floating-header-share">
        <div class="floating-header-share-label">이 포스트를 공유하세요 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Hadoop+%EC%84%A4%EC%B9%98%EC%99%80+%ED%99%9C%EC%9A%A9&amp;url=https://moon9342.github.io/hadoop"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://moon9342.github.io/hadoop"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>


    <!-- floating-header에 이미지 삽입 -->
<!--
    <div class="floating-header-share">
        <img src="assets/images/photo.jpg" width="50">
    </div>
-->

    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->

        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">IT Technical Training</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/">GitHub Pages</a></section>
                <nav class="site-footer-nav">
                    <a href="mailto:moon9342@gmail.com">moon9342@gmail.com</a>
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search IT Technical Training</h1>
                <p class="subscribe-overlay-description">lunr.js를 이용한 posts 검색 </p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  /><input class="location" type="hidden" name="location"  /><input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" id="searchtext" type="text" name="searchtext"  placeholder="Search..." />
    </div>
    <!--<button class="" onclick="myFunc()" id="searchBtn" type="button" ><span>Search</span></button>-->
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
    <!--<script type="text/et-cetera">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>-->
</span>


            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113135179-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
